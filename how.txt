#include <iostream>
#include <stdlib.h>
#include <stdio.h>
using namespace std;
#define SIZE 10

//__global__ says that function is a kernel which will be executed by GPU by one or more simultaneous thread
// must return void, pass things by reference if you wan value
// only callable form cpu
//threadIDx.x -> within current blk id of current thread in x direction [0 - (num of thread per blk-1) ]
//blockIdx.x -> id of current blk [0 - (numof blks -1) ]
//gridDim.x -> no of blks in x direction in current grid
//blockDim.x -> no of thread in x direction in current blk
__global__ void compute(int *v1,int *v2, int *v3, int N){

	__shared__ int temp[SIZE];	//shared memory is allocated per thread block. so all threads will have access to same shared memory


	__syncthreads();	//is a block level sync barrier. 
						//waits until all threads within same block has readched this command
						//this to avoid race condition, when there is shared data b/w threads 
}

// __host__ says function will be executed by host, all function without prefex are also host function
//only callable from cpu
__host__ void compute2(){    }

//__device__ says function will be executed by kernel once per call
//can be called from gpu only inside kernel or another device function
__device__ void compute3(){   }


int main(){
	int *var, *var_d;
	v1 = (int*)malloc(SIZE);
	
	//intialize variable

	//intialize memory on 
	cudaMalloc((void**)&var_d,SIZE;
	
	//copy data from host to device
	cudaMemcpy(var_d,var,SIZE,cudaMemcpyHostToDevice);


	//COMPUTE<<< NO_OF_BLKS , THREADS_PER_BLK >>>(var_d)

	cudaMemcpy(v3,v3_d,sizeof(int)*300,cudaMemcpyDeviceToHost);
	for(int i=0;i<N;i++){
		cout<<v3[i]<<"\t"<<v3[i+N]<<"\t"<<v3[i+N*2]<<endl;
	}
	cout<<"done";
	return 0;
}

/*
 WORK FLOW

 declare variable
 allocate host memory
 allocate device memory for gpu results
 write to host memory
 copy from hot to device
 execute kernel
 write gpu results in hot memory
 free host memory
 free device memory
*/
